{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import wfdb\n",
    "\n",
    "import os\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infarction Stadium Labels\n",
    "Infarction Stadium details how damaged the heart tissue is.\n",
    "* I used the methodology described in this paper to distill the labels: https://arxiv.org/pdf/2306.15681"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Given a 12 lead ECG system, classify the infarction stadium level.\n",
    "* Noise may or may not be present in the signal\n",
    "\n",
    "# Strategy\n",
    "* First, train a neural network (NN) to identify the four types of noise ['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems']\n",
    "* Next, train a neural network to identify the infarction stadium (IS) using the 12 lead signal information with an additional layer than indicates what type of noise is in the signal for each lead.\n",
    "* In Production, each lead is passed through the noise detection NN to detect noise. That information is then passed to the IS neural network with the signals to make a classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory_path = '/home/complukter/Desktop/PTBXL-data'\n",
    "sig_database = pd.read_csv(os.path.join(root_directory_path, 'ptbxl_database.csv'), index_col='ecg_id')\n",
    "sig_database.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the stuff we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_database = sig_database.drop(\n",
    "    ['patient_id', 'age', 'sex', 'height', 'weight', 'nurse', 'site', 'device', 'recording_date', 'validated_by', 'second_opinion', 'initial_autogenerated_report', 'validated_by_human','extra_beats', 'pacemaker'],\n",
    "    axis=1\n",
    ")\n",
    "print(sig_database.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at one Signal\n",
    "* We will use the 100 hz sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_one_sample(path):\n",
    "    data = wfdb.rdsamp(path)\n",
    "    signals = np.asmatrix(data[0])\n",
    "    meta = data[1]\n",
    "    return signals, meta\n",
    "\n",
    "one_sample = sig_database.iloc[0]\n",
    "file_path = one_sample['filename_lr']\n",
    "signals, meta = load_one_sample(os.path.join(root_directory_path, file_path))\n",
    "print(signals, meta)\n",
    "signal = signals.flatten().tolist()[0]\n",
    "print(signal)\n",
    "plt.plot(range(len(signal)), signal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Identification Dataset\n",
    "The issue with this dataset is that there is lots of noise in the signals. Thankfully, the 4 types of noise have been tagged by the lead. I want to clean up this dataset and create a noise classifier, however, there is a lot of garbage labelings. The goal is to first make a pipeline that can parse the strings and map them to the leads, and for the examples where it's not clear,  look at them individually. Additionally, tag the clean ones as clean.\n",
    "\n",
    "Assumptions we are making:\n",
    "- the ';' is replacable with a ',' not a '-'. Meaning it does not represent a range.\n",
    "- Any other strings other than lead names, 'nan' or 'alles' will be rejected from the dataset\n",
    "- lead order in the 'lead_table' comes from the signal meta data from one of the signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_table = ['i', 'ii', 'iii', 'avr', 'avl', 'avf', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6']\n",
    "error_message = 'erroneous'\n",
    "clean_message = 'clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lead_symbols(lead_string):\n",
    "\n",
    "    # if the string is empty, we want to tag it as clean\n",
    "    lead_string = str(lead_string).lower()\n",
    "\n",
    "    if lead_string == 'nan':\n",
    "        return clean_message\n",
    "    \n",
    "    lead_string = lead_string.replace(';', ',')\n",
    "        \n",
    "    strings = [ s.strip().replace(' ', '') for s in str(lead_string).split(',') if s.replace(' ', '') != '']\n",
    "    final_symbols = []\n",
    "    for s in strings:\n",
    "\n",
    "        # All values\n",
    "        if s == 'alles':\n",
    "            return lead_table\n",
    "        \n",
    "        # Dash Present\n",
    "        elif '-' in s:\n",
    "            symbol_range = s.split('-')\n",
    "\n",
    "            # check if the second value is just a number, if so, add a 'v' to the front of it\n",
    "            if symbol_range[-1].isdigit():\n",
    "                symbol_range[-1] = 'v' + symbol_range[-1]\n",
    "\n",
    "            # Check if the range is possible, if not, then it is garbage\n",
    "            if symbol_range[0] in lead_table and symbol_range[-1] in lead_table:\n",
    "                start_index, end_index = lead_table.index(symbol_range[0]), lead_table.index(symbol_range[-1])\n",
    "                symbols = lead_table[start_index: end_index + 1]\n",
    "                final_symbols.extend(symbols)\n",
    "\n",
    "            ####### Garbage\n",
    "            else:\n",
    "                return error_message\n",
    "            \n",
    "        # no dash\n",
    "        else:\n",
    "            if s in lead_table:\n",
    "                final_symbols.append(s)\n",
    "\n",
    "            ####### Garbage\n",
    "            else:\n",
    "                return error_message\n",
    "\n",
    "    return final_symbols if len(final_symbols) else error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that were erroneous\n",
    "sig_database[['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems']] = sig_database[['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems']].map(lambda x: filter_lead_symbols(x))\n",
    "mask = sig_database[['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems']].map(lambda x: error_message in str(x)).any(axis=1)\n",
    "sig_database = sig_database[~mask]\n",
    "len(sig_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At this point...\n",
    "We have parsed the lead information for the four types of noise present in the signals and have removed the unparsable examples from the dataset. Now we will create a dataset where each example is a signal and the label is 5 element array indicating the presence of the noise, for example:\n",
    "\n",
    "# Format ['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems', 'clean'] could translate to [0, 1, 1, 0, 0] to indicate 'baseline_drift' and 'static noise'\n",
    "- If there is any noise whatsoever, the clean flag cannot be set.\n",
    "\n",
    "The way the signal data is stored is in a (10 sec * sample_rate) length array of length 12 arrays, where each index in the 12 value array refers to a specific lead. We first need to flip the array to be oriented the other way, then create the labels and export the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform(df):\n",
    "    file_path = df['filename_lr']\n",
    "    signals, meta = load_one_sample(os.path.join(root_directory_path, file_path))\n",
    "    signals = signals.T # transpose to get each individual signal on it's own row\n",
    "    noise_types = ['baseline_drift', 'static_noise', 'burst_noise', 'electrodes_problems', 'clean'] # The last column is Clean\n",
    "\n",
    "    classifications = np.zeros((signals.shape[0], len(noise_types)))    # This will store the classifications data\n",
    "    for noise_index, type in enumerate(noise_types):\n",
    "        if type == 'clean':\n",
    "            break\n",
    "        # Get the indexes of the signals that contain this type of noise using the lead code\n",
    "        codes_for_noise_type = df[type]\n",
    "\n",
    "        # No Leads have noise\n",
    "        if codes_for_noise_type != 'clean':\n",
    "            for code in codes_for_noise_type:\n",
    "                lead_index = lead_table.index(code)\n",
    "                classifications[lead_index][noise_index] = 1\n",
    "\n",
    "    # If no noise it present in the column, mark the clean label as true\n",
    "    for lead_index in range(classifications.shape[0]):\n",
    "        if np.sum(classifications[lead_index: -1]) == 0.0:\n",
    "            classifications[lead_index][-1] = 1\n",
    "\n",
    "    # Export the signal, label matchings\n",
    "    rows = []\n",
    "    for lead_index in range(signals.shape[0]):\n",
    "        rows.append({'signal': signals[lead_index].tolist()[0], 'label' : classifications[lead_index]})\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "load_and_transform(sig_database.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this transform to the entire dataset, producing a new dataframe, export it to a file\n",
    "# Initialize an empty list to collect all rows\n",
    "\n",
    "with open('noisey_signal_dataset.csv', 'w') as output_file:\n",
    "\n",
    "    # Use tqdm to monitor the progress\n",
    "    for index, row in tqdm.tqdm(sig_database.iterrows(), total=sig_database.shape[0], desc=\"Processing rows\"):\n",
    "        formatted_data = load_and_transform(row)\n",
    "\n",
    "        formatted_data.columns = ['signal', 'classification']\n",
    "        formatted_data.to_csv(output_file, mode='a', header=(index == 0), index=False)\n",
    "\n",
    "        # Drop the row to free up space\n",
    "        sig_database.drop(index, inplace=True)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
